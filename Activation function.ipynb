{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d497fbee-4bb0-4636-a488-108012cd3059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f0431-be96-45cd-a181-bea7465df43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Sigmoid Activation Function:\n",
    "The sigmoid function maps the input to a value between 0 and 1, which can be interpreted as a probability.\n",
    "It is commonly used in the output layer for binary classification problems.\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def sigmoid_activation(x):\n",
    "    return tf.sigmoid(x)\n",
    "\n",
    "# Example usage\n",
    "input_data = tf.constant([1.0, 2.0, 3.0])\n",
    "output = sigmoid_activation(input_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0997eaae-a15c-4d91-9f40-74afb936701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Rectified Linear Unit (ReLU) Activation Function:\n",
    "ReLU function returns the input value if it is positive, otherwise, it returns zero. \n",
    "It is widely used in hidden layers of deep neural networks due to its simplicity and ability to alleviate the vanishing gradient problem.\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def relu_activation(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "# Example usage\n",
    "input_data = tf.constant([-1.0, 2.0, -3.0])\n",
    "output = relu_activation(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9124a-1a7f-4d18-8feb-8d70b34feb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Hyperbolic Tangent (tanh) Activation Function:\n",
    "The tanh function is similar to the sigmoid function but maps the input to a value between -1 and 1. It is often used in hidden layers of neural networks.\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def tanh_activation(x):\n",
    "    return tf.nn.tanh(x)\n",
    "\n",
    "# Example usage\n",
    "input_data = tf.constant([-1.0, 2.0, -3.0])\n",
    "output = tanh_activation(input_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3f715-223c-4b12-b3e9-f303f41692a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d20a7d-d266-44d3-91c2-52f4f1b8ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are several common types of activation functions used in neural networks. Here are a few of them:\n",
    "\n",
    "1. Sigmoid Activation Function:\n",
    "   The sigmoid function, also known as the logistic function, maps the input to a value between 0 and 1. It has an S-shaped curve and is useful in models that require binary classification or probabilistic outputs.\n",
    "\n",
    "2. Hyperbolic Tangent (tanh) Activation Function:\n",
    "   The tanh function is similar to the sigmoid function but maps the input to a value between -1 and 1. It is also an S-shaped curve but symmetric around the origin. \n",
    "   Tanh is commonly used in models where the output can have negative values.\n",
    "\n",
    "3. Rectified Linear Unit (ReLU) Activation Function:\n",
    "   The ReLU function returns the input value if it is positive, otherwise, it returns zero.\n",
    "   It is a simple and computationally efficient activation function that has been widely used in deep learning models. \n",
    "    ReLU helps alleviate the vanishing gradient problem and promotes sparsity in neural networks.\n",
    "\n",
    "4. Leaky ReLU Activation Function:\n",
    "   The Leaky ReLU is a variation of the ReLU function that allows small negative values for inputs below zero.\n",
    "It is defined as f(x) = max(ax, x), where 'a' is a small positive constant. \n",
    "The purpose of the leaky ReLU is to overcome the \"dying ReLU\" problem where neurons become inactive and stop learning.\n",
    "\n",
    "5. Parametric ReLU (PReLU) Activation Function:\n",
    "   PReLU is similar to Leaky ReLU but allows the 'a' parameter to be learned during training instead of being predefined. \n",
    "It introduces additional parameters to the model, providing more flexibility.\n",
    "\n",
    "6. Softmax Activation Function:\n",
    "   Softmax is commonly used in multi-class classification problems. It transforms the input into a probability distribution over multiple classes, where the sum of the probabilities is equal to 1. Softmax is often used in the output layer of a neural network for multi-class classification tasks.\n",
    "\n",
    "These are some of the most commonly used activation functions in neural networks. \n",
    "The choice of activation function depends on the specific problem, network architecture, and characteristics of the data being modeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed1a56f-b059-4148-88f7-a7ee9084fdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfc8025-8888-4181-81c8-0fb872236666",
   "metadata": {},
   "outputs": [],
   "source": [
    "Activation functions play a crucial role in the training process and performance of a neural network. Here's how activation functions can affect the network:\n",
    "\n",
    "1. Non-Linearity and Representation Power:\n",
    "   Activation functions introduce non-linearity to the network, allowing it to learn and represent complex patterns in the data.\n",
    "   Without non-linear activation functions, a neural network would be equivalent to a linear model, limiting its ability to capture intricate relationships in the data.\n",
    "    By using non-linear activation functions, the network becomes capable of modeling highly nonlinear mappings between inputs and outputs.\n",
    "\n",
    "Example:\n",
    "   Let's consider a simple classification problem where the data points are not linearly separable. If we use a linear activation function such as the identity function, the neural network would fail to learn the decision boundary. However, by applying a non-linear activation function like the sigmoid or ReLU, the network can learn the complex decision boundary and achieve higher classification accuracy.\n",
    "\n",
    "2. Gradient Flow and Backpropagation:\n",
    "   During the training process, neural networks use backpropagation to update the weights based on the calculated gradients. \n",
    "The choice of activation function can impact the flow of gradients through the network. If the gradients become too small or vanish, it can lead to the \"vanishing gradient\" problem, where the network fails to learn effectively.\n",
    "\n",
    "   Activation functions like sigmoid and tanh can suffer from vanishing gradients, especially for large inputs, making it harder for the network to propagate useful information backward.\n",
    "    On the other hand, activation functions like ReLU and its variants (Leaky ReLU, PReLU) tend to have better gradient flow as they don't saturate for positive inputs.\n",
    "\n",
    "Example:\n",
    "Consider a deep neural network with many layers.\n",
    "If we use a sigmoid activation function throughout the network, the gradients might become extremely small as they propagate backward, making it difficult for the earlier layers to update their weights effectively. \n",
    "In contrast, using ReLU or its variants can mitigate the vanishing gradient problem, enabling better gradient flow and more efficient training.\n",
    "\n",
    "3. Sparsity and Generalization:\n",
    "   Activation functions like ReLU can induce sparsity in the network by zeroing out negative activations.\n",
    "This sparsity property helps in feature selection and reduces the number of active neurons.\n",
    "Sparse networks can have better generalization performance as they are less prone to overfitting, as well as being computationally efficient.\n",
    "\n",
    "Example:\n",
    "   In a convolutional neural network (CNN) used for image classification, ReLU activation can help identify and activate only the relevant features in each layer while suppressing less informative activations. \n",
    "This sparse representation can lead to better generalization and more efficient inference.\n",
    "\n",
    "It's important to choose an appropriate activation function based on the characteristics of the problem, the network architecture, and the nature of the data. Different activation functions have different properties and can impact the training process, convergence speed, and generalization performance of the neural network.\n",
    "Experimenting with different activation functions is often necessary to find the most suitable one for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4366c55c-3136-431c-89de-1059b8225d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2b1874-ccb1-412d-b062-e8b8dfee733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "The rectified linear unit (ReLU) activation function is a non-linear function commonly used in neural networks, especially in deep learning models. It differs significantly from the sigmoid function in terms of its behavior and properties.\n",
    "\n",
    "The ReLU activation function is defined as:\n",
    "\n",
    "f(x) = max(0, x)\n",
    "\n",
    "Here's how the ReLU activation function works and how it differs from the sigmoid function:\n",
    "\n",
    "1. Range and Linearity:\n",
    "   The ReLU function returns the input value if it is positive, and zero otherwise. It effectively \"rectifies\" negative values to zero. Unlike the sigmoid function, ReLU is a piecewise linear function that remains linear for positive inputs. This linearity contributes to the efficiency of training and computation.\n",
    "\n",
    "2. Non-Linearity and Sparse Activation:\n",
    "   While ReLU is linear for positive inputs, it introduces non-linearity by setting all negative values to zero. This non-linearity enables the neural network to learn complex, non-linear relationships in the data. Additionally, the sparsity induced by ReLU activation leads to sparse activation patterns, where only a subset of neurons are activated for each input, promoting efficient computation and feature selection.\n",
    "\n",
    "3. Avoiding Vanishing Gradients:\n",
    "   ReLU addresses the vanishing gradient problem that can occur with the sigmoid function. When the input to the sigmoid function is very large or very small, the gradient approaches zero, hindering the flow of gradients during backpropagation. In contrast, ReLU has a constant gradient of 1 for positive inputs, making gradient propagation more efficient and avoiding the vanishing gradient issue.\n",
    "\n",
    "4. Output Range:\n",
    "   Unlike the sigmoid function, ReLU does not constrain the output within a specific range. The output of ReLU can be any positive value or zero. This unbounded nature of the ReLU activation function can be advantageous in certain cases, such as regression tasks where the output can be any real value.\n",
    "\n",
    "To summarize, the main differences between the ReLU activation function and the sigmoid function are:\n",
    "\n",
    "- Range: ReLU outputs values that are either positive or zero, while sigmoid outputs values between 0 and 1.\n",
    "- Linearity: ReLU is linear for positive inputs, while sigmoid is non-linear throughout its range.\n",
    "- Non-linearity: ReLU introduces non-linearity by setting negative values to zero, whereas sigmoid has a smooth S-shaped non-linear curve.\n",
    "- Vanishing Gradients: ReLU helps alleviate the vanishing gradient problem that can occur with the sigmoid function.\n",
    "\n",
    "ReLU has become widely popular in deep learning due to its simplicity, computational efficiency, and ability to mitigate the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb0015-2c69-4b7b-97b5-0f45f51da3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5383aa3-49d2-42b1-bf6c-c8a0a89e92b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "The sigmoid activation function, also known as the logistic function, maps the input to a value between 0 and 1. It is defined as:\n",
    "\n",
    "f(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "Here's how the sigmoid activation function works:\n",
    "\n",
    "1. Range and Interpretation:\n",
    "   The sigmoid function squashes the input to a range between 0 and 1. It transforms any real-valued input into a probability-like output. The output can be interpreted as the probability of the neuron being activated or firing.\n",
    "\n",
    "2. Non-Linearity:\n",
    "   The sigmoid function introduces non-linearity to the neural network. It allows the network to model and learn complex, non-linear relationships between inputs and outputs.\n",
    "\n",
    "3. Smoothness and Differentiability:\n",
    "   The sigmoid function is smooth and differentiable for all input values. This property is essential for gradient-based optimization algorithms like backpropagation, which rely on derivatives to update the network's weights during training.\n",
    "\n",
    "Advantages of the sigmoid activation function:\n",
    "\n",
    "1. Interpretability:\n",
    "   The sigmoid function maps the output to a value between 0 and 1, which can be interpreted as a probability or confidence score. It is commonly used in binary classification tasks where the goal is to estimate the probability of an input belonging to a certain class.\n",
    "\n",
    "2. Smoothness:\n",
    "   The sigmoid function is a smooth and continuous function, making it differentiable at all points. This property helps with gradient-based optimization algorithms during training, allowing the network to efficiently update its weights.\n",
    "\n",
    "Disadvantages of the sigmoid activation function:\n",
    "\n",
    "1. Vanishing Gradient:\n",
    "   The sigmoid function suffers from the \"vanishing gradient\" problem, especially for large inputs. As the input moves away from zero, the derivative of the sigmoid becomes very small, which leads to vanishing gradients during backpropagation. Vanishing gradients can hinder the training process, making it harder for the network to learn.\n",
    "\n",
    "2. Output Saturation:\n",
    "   The sigmoid function saturates at both ends of the input range, causing the output to be close to 0 or 1. This saturation means that large changes in the input result in only small changes in the output. It can slow down the learning process, particularly when the network is far from the optimal solution.\n",
    "\n",
    "3. Not Zero-Centered:\n",
    "   The sigmoid function is not zero-centered, as its output is always positive. This property can introduce a bias in the gradients during backpropagation, which can affect the convergence of the network.\n",
    "\n",
    "Due to these limitations, the sigmoid activation function is less commonly used in hidden layers of deep neural networks. It is often replaced by activation functions like ReLU and its variants, which address the vanishing gradient problem and offer faster convergence. However, the sigmoid function can still be useful in the output layer for binary classification problems or tasks where the output needs to be interpreted as a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa88ad-45d6-4ea3-bbb1-6ffd45ad1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1fcd36-d956-4315-a217-f5e21d8bf53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using the ReLU (Rectified Linear Unit) activation function instead of the sigmoid function offers several benefits in neural networks. Here are some of the key advantages of ReLU:\n",
    "\n",
    "1. Addressing Vanishing Gradient:\n",
    "   The ReLU activation function helps mitigate the vanishing gradient problem that can occur with the sigmoid function. In deep neural networks, gradients can diminish exponentially as they propagate backward through the layers, making it challenging for earlier layers to learn effectively. ReLU has a constant gradient of 1 for positive inputs, allowing gradients to flow more freely and avoiding the vanishing gradient issue. This enables better learning and faster convergence.\n",
    "\n",
    "2. Improved Training Speed:\n",
    "   ReLU's piecewise linearity and non-saturating nature make it computationally efficient compared to sigmoid. The activation is simply a threshold operation, setting negative values to zero. This simplicity speeds up both the forward and backward pass computations, leading to faster training of neural networks, especially in deep architectures.\n",
    "\n",
    "3. Sparse Activation and Feature Selection:\n",
    "   ReLU activation encourages sparsity by setting negative values to zero. As a result, ReLU-based networks tend to have sparse activation patterns, where only a subset of neurons are activated for each input. Sparse activations offer benefits such as reduced memory usage, computational efficiency, and improved generalization. Additionally, the sparsity induced by ReLU can act as a form of feature selection, as only relevant features are activated, allowing the network to focus on the most informative parts of the input.\n",
    "\n",
    "4. Avoiding Saturation:\n",
    "   Unlike sigmoid, ReLU does not saturate at the upper end of its range. Sigmoid can reach saturation, causing the gradient to become very small, especially for large positive or negative inputs. ReLU's linear behavior for positive inputs avoids this saturation issue, allowing for better gradient flow and preventing the network from getting stuck in saturated regions.\n",
    "\n",
    "5. Better Representational Capacity:\n",
    "   ReLU provides a more expressive activation than sigmoid due to its piecewise linearity. The linear behavior for positive inputs allows ReLU to model a wider range of non-linear functions compared to sigmoid. This increased representational capacity can be beneficial for complex learning tasks and can improve the overall performance of the network.\n",
    "\n",
    "These advantages have made ReLU a popular choice for activation functions, particularly in deep learning models. It has demonstrated superior performance and faster convergence in various tasks, enabling the training of deeper and more powerful neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8475cc11-a87f-4866-99d8-aacaef2fba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d3aa4-9bef-4671-9b63-8851c15fea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Leaky ReLU (Rectified Linear Unit) is a variation of the ReLU activation function that addresses the vanishing gradient problem associated with traditional ReLU. It introduces a small slope for negative input values instead of setting them to zero, allowing a small, non-zero gradient to flow during backpropagation. This small slope prevents complete saturation and helps alleviate the dying ReLU problem where neurons can become inactive and stop learning.\n",
    "\n",
    "The Leaky ReLU function is defined as:\n",
    "\n",
    "f(x) = max(ax, x)\n",
    "\n",
    "where 'a' is a small positive constant (usually a small value like 0.01).\n",
    "\n",
    "Here's how the Leaky ReLU works and how it addresses the vanishing gradient problem:\n",
    "\n",
    "1. Linearity for Positive Inputs:\n",
    "   Similar to the ReLU function, the Leaky ReLU keeps positive inputs unchanged, resulting in a linear activation with a slope of 1. This linearity allows the network to learn more easily and efficiently for positive input values.\n",
    "\n",
    "2. Non-Zero Gradient for Negative Inputs:\n",
    "   Unlike the ReLU function, the Leaky ReLU does not set negative inputs to zero. Instead, it introduces a small slope defined by the constant 'a' for negative inputs. This small non-zero slope ensures that a gradient can flow during backpropagation, even for negative inputs. By providing a non-zero gradient, the Leaky ReLU helps to prevent the vanishing gradient problem and enables neurons to continue updating their weights and learning from negative input values.\n",
    "\n",
    "3. Preventing Neuron Inactivity:\n",
    "   The small slope introduced by the Leaky ReLU ensures that neurons never completely \"die\" or become inactive. With a non-zero gradient for negative inputs, even neurons that receive strongly negative inputs can still contribute to learning and adjust their weights during training. This property helps in maintaining the expressive power of the neural network, especially in deeper architectures.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a deep neural network with multiple layers, using Leaky ReLU as the activation function. During the forward pass, if a neuron receives a negative input, instead of setting it to zero like in ReLU, Leaky ReLU applies a small slope to the negative input, allowing a non-zero value to be propagated through the network.\n",
    "\n",
    "For instance, if 'a' is set to 0.01, the Leaky ReLU function for a negative input of -5 would result in an output of -0.05 (-5 * 0.01). This small negative value ensures that the gradient during backpropagation remains non-zero, allowing the network to continue learning from negative inputs.\n",
    "\n",
    "By introducing this slight leakiness, the Leaky ReLU activation function helps to mitigate the vanishing gradient problem and maintain the learning capacity of the network even with negative inputs, contributing to better overall performance and training stability in deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3471be00-00c8-403e-8745-d5a291088091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7354fb0d-e884-4451-a269-d349a9ad4e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "The softmax activation function is primarily used in multi-class classification tasks, where the goal is to assign an input to one of multiple classes. It converts a vector of real numbers into a probability distribution over the classes, allowing the network to produce class probabilities that sum to 1. The softmax function is defined as:\n",
    "\n",
    "softmax(x_i) = exp(x_i) / sum(exp(x_j))\n",
    "\n",
    "where x_i represents the i-th element of the input vector x, and the sum is taken over all elements in x.\n",
    "\n",
    "The purpose of the softmax activation function can be summarized as follows:\n",
    "\n",
    "1. Probability Distribution:\n",
    "   The softmax function transforms the outputs of a neural network into a probability distribution over multiple classes. \n",
    "It ensures that the predicted probabilities are non-negative and sum to 1, representing the confidence or likelihood of the input belonging to each class.\n",
    "\n",
    "2. Class Selection:\n",
    "   By converting the network's outputs into probabilities, softmax allows for straightforward class selection. The class with the highest probability is typically chosen as the predicted class label.\n",
    "Softmax outputs can be directly interpreted as class probabilities, enabling easy decision-making in classification tasks.\n",
    "\n",
    "3. Training with Cross-Entropy Loss:\n",
    "   Softmax is often used in conjunction with the cross-entropy loss function during training. \n",
    "The combination of softmax and cross-entropy loss enables the network to learn to produce high probabilities for the correct class and low probabilities for incorrect classes.\n",
    "The network learns to minimize the cross-entropy loss, which measures the dissimilarity between the predicted probabilities and the true class labels.\n",
    "\n",
    "The softmax activation function is commonly used in the output layer of neural networks for multi-class classification problems.\n",
    "It is suitable when the number of classes is greater than two.\n",
    "Some examples of applications where softmax is commonly used include image classification, text classification, speech recognition, and sentiment analysis.\n",
    "\n",
    "It's important to note that softmax is not suitable for binary classification tasks, where only two classes are involved. For binary classification, a different activation function like sigmoid is typically used in the output layer, as it provides a single probability value for each class independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dcc271-be41-4c7a-89be-1d41823b5e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6c1eb-e1d4-4adf-988b-9948c3227715",
   "metadata": {},
   "outputs": [],
   "source": [
    "The hyperbolic tangent (tanh) activation function is a non-linear function that maps the input to a value between -1 and 1. It is defined as:\n",
    "\n",
    "tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "Here's how the tanh activation function works and how it compares to the sigmoid function:\n",
    "\n",
    "1. Range and Symmetry:\n",
    "   The tanh function maps the input to a value between -1 and 1, making it zero-centered.\n",
    "It is similar to the sigmoid function in terms of the S-shaped curve, but with a range from -1 to 1 instead of 0 to 1. The output of tanh is negative for negative inputs and positive for positive inputs, resulting in a symmetric activation function around the origin.\n",
    "\n",
    "2. Non-Linearity:\n",
    "   Like the sigmoid function, tanh introduces non-linearity to the neural network. \n",
    "It allows the network to model and learn complex, non-linear relationships between inputs and outputs.\n",
    "However, tanh has steeper gradients around zero compared to the sigmoid, making it a slightly stronger non-linearity.\n",
    "\n",
    "3. Output Sensitivity:\n",
    "   The tanh function has higher output sensitivity to small changes around zero compared to the sigmoid function.\n",
    "This means that it produces larger gradients for inputs close to zero, making it more sensitive to variations in the input.\n",
    "However, like the sigmoid function, tanh also saturates at the extreme ends of the input range, resulting in smaller gradients for large positive or negative inputs.\n",
    "\n",
    "4. Zero-Centered:\n",
    "   One advantage of the tanh function over the sigmoid function is that it is zero-centered. \n",
    "The output of tanh is symmetric around zero, meaning that the average output of the tanh activations will be close to zero.\n",
    "This property can be beneficial in situations where zero-centered data or gradients are desired, making it easier for the network to learn and converge.\n",
    "\n",
    "5. Vanishing Gradient:\n",
    "   The tanh function still suffers from the vanishing gradient problem, albeit to a lesser extent than the sigmoid function. \n",
    "The gradient of tanh decreases as the input moves away from zero, but it saturates less quickly compared to sigmoid.\n",
    "It can still encounter vanishing gradients for very large or small inputs, affecting the training process, especially in deep architectures.\n",
    "\n",
    "In summary, the main differences between the tanh and sigmoid activation functions are the output range (-1 to 1 for tanh, 0 to 1 for sigmoid) and the zero-centered nature of tanh. \n",
    "Tanh provides a stronger non-linearity and is useful in situations where zero-centered data or gradients are desired. However, both functions can encounter vanishing gradients for extreme inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5268186-a122-4696-b201-9179b5a48cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6ca585-b8b7-493b-a5d2-79c325272c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11664b-69ae-4ef1-805d-d383054f5f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2d96b8-895e-49b5-82fa-7833f239e5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118efaeb-6010-4267-a5bc-dd8bf55919f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfaf400-8fed-4305-8fe2-a8fa7d666508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
